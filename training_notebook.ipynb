{"cells":[{"cell_type":"code","execution_count":2,"id":"1ee66840","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting pyspark_dist_explore\n","  Downloading pyspark_dist_explore-0.1.8-py3-none-any.whl (7.2 kB)\n","Requirement already satisfied: pandas in /opt/conda/miniconda3/lib/python3.8/site-packages (from pyspark_dist_explore) (1.2.5)\n","Requirement already satisfied: numpy in /opt/conda/miniconda3/lib/python3.8/site-packages (from pyspark_dist_explore) (1.19.5)\n","Requirement already satisfied: scipy in /opt/conda/miniconda3/lib/python3.8/site-packages (from pyspark_dist_explore) (1.6.3)\n","Requirement already satisfied: matplotlib in /opt/conda/miniconda3/lib/python3.8/site-packages (from pyspark_dist_explore) (3.4.3)\n","Requirement already satisfied: cycler>=0.10 in /opt/conda/miniconda3/lib/python3.8/site-packages (from matplotlib->pyspark_dist_explore) (0.11.0)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/miniconda3/lib/python3.8/site-packages (from matplotlib->pyspark_dist_explore) (1.4.4)\n","Requirement already satisfied: pillow>=6.2.0 in /opt/conda/miniconda3/lib/python3.8/site-packages (from matplotlib->pyspark_dist_explore) (9.2.0)\n","Requirement already satisfied: pyparsing>=2.2.1 in /opt/conda/miniconda3/lib/python3.8/site-packages (from matplotlib->pyspark_dist_explore) (2.4.7)\n","Requirement already satisfied: python-dateutil>=2.7 in /opt/conda/miniconda3/lib/python3.8/site-packages (from matplotlib->pyspark_dist_explore) (2.8.0)\n","Requirement already satisfied: pytz>=2017.3 in /opt/conda/miniconda3/lib/python3.8/site-packages (from pandas->pyspark_dist_explore) (2023.3)\n","Requirement already satisfied: six>=1.5 in /opt/conda/miniconda3/lib/python3.8/site-packages (from python-dateutil>=2.7->matplotlib->pyspark_dist_explore) (1.16.0)\n","Installing collected packages: pyspark_dist_explore\n","Successfully installed pyspark_dist_explore-0.1.8\n","\u001B[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001B[0m\u001B[33m\n","\u001B[0m"]}],"source":["!pip install pyspark_dist_explore"]},{"cell_type":"code","execution_count":3,"id":"9977be3e","metadata":{},"outputs":[],"source":["import pyspark.sql.functions as F\n","from pyspark.sql.functions import udf, col,countDistinct\n","from pyspark.ml.feature import StringIndexer,VectorAssembler,Normalizer,VectorIndexer\n","from pyspark.ml.classification import LogisticRegression,RandomForestClassifier,GBTClassifier,NaiveBayes\n","from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n","from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n","from pyspark.ml import Pipeline\n","from pyspark.sql import SparkSession\n","from pyspark.ml.linalg import SparseVector, DenseVector\n","from pyspark.sql.functions import col, udf\n","from pyspark.ml.linalg import Vectors, VectorUDT"]},{"cell_type":"code","execution_count":4,"id":"71970cc9","metadata":{},"outputs":[],"source":["import warnings\n","warnings.simplefilter(\"ignore\")"]},{"cell_type":"code","execution_count":5,"id":"7051f0bd","metadata":{},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import pandas as pd"]},{"cell_type":"code","execution_count":6,"id":"aaf53e9e","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Setting default log level to \"WARN\".\n","To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n","23/04/27 21:02:54 INFO org.apache.spark.SparkEnv: Registering MapOutputTracker\n","23/04/27 21:02:54 INFO org.apache.spark.SparkEnv: Registering BlockManagerMaster\n","23/04/27 21:02:54 INFO org.apache.spark.SparkEnv: Registering BlockManagerMasterHeartbeat\n","23/04/27 21:02:54 INFO org.apache.spark.SparkEnv: Registering OutputCommitCoordinator\n"]}],"source":["spark = SparkSession.builder.appName('final_project_EDA').getOrCreate()"]},{"cell_type":"code","execution_count":7,"id":"b6b3360d","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]}],"source":["file_location ='gs://mm19b059_1/trainingdatanyc.csv'\n","#dataset = spark.read.format('csv').option('header',True).option('inferSchema',True).load(file_location)\n","dataset = spark.read.options(delimiter=\",\",header=True, inferSchema = True).csv(file_location).limit(1000000)"]},{"cell_type":"code","execution_count":8,"id":"2b8af5a8","metadata":{},"outputs":[],"source":["#dataset.select('*').limit(5).show()"]},{"cell_type":"code","execution_count":9,"id":"22619a11","metadata":{},"outputs":[],"source":["#dataset.printSchema()"]},{"cell_type":"code","execution_count":10,"id":"dc264f6b","metadata":{},"outputs":[],"source":["#dataset.count()"]},{"cell_type":"code","execution_count":11,"id":"c8b6e453","metadata":{},"outputs":[],"source":["#len(dataset.columns)"]},{"cell_type":"code","execution_count":12,"id":"ec84ba06","metadata":{},"outputs":[],"source":["dataset=dataset.dropDuplicates()\n","#dataset.count()"]},{"cell_type":"code","execution_count":13,"id":"a341912f","metadata":{},"outputs":[],"source":["dataframe = dataset.toDF(*(c.replace(' ', '_') for c in dataset.columns))"]},{"cell_type":"code","execution_count":14,"id":"9c035ce0","metadata":{"scrolled":true},"outputs":[{"data":{"text/plain":["\"from pyspark_dist_explore import hist\\n\\nfig, ax = plt.subplots()\\nhist(ax, dataframe.select('Violation_Precinct'), bins = 200, color=['red'])\\nplt.show()\""]},"execution_count":14,"metadata":{},"output_type":"execute_result"}],"source":["'''from pyspark_dist_explore import hist\n","\n","fig, ax = plt.subplots()\n","hist(ax, dataframe.select('Violation_Precinct'), bins = 200, color=['red'])\n","plt.show()'''"]},{"cell_type":"code","execution_count":15,"id":"63e28d55","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Numeric columns ['Feet_From_Curb', 'Issuer_Precinct', 'Street_Code2', 'Street_Code1', 'Issuer_Code', 'Street_Code3', 'Violation_Code', 'Vehicle_Year', 'Summons_Number', 'Law_Section', 'Violation_Location']\n","----------------------------------------------------------------------------------------------------\n","categorical columns ['Violation_Time', 'Violation_In_Front_Of_Or_Opposite', 'From_Hours_In_Effect', 'Issuing_Agency', 'Violation_County', 'Meter_Number', 'Plate_Type', 'Unregistered_Vehicle?', 'Issue_Date', 'Violation_Post_Code', 'Double_Parking_Violation', 'Plate_ID', 'Street_Name', 'Registration_State', 'Hydrant_Violation', 'Days_Parking_In_Effect', 'Vehicle_Expiration_Date', 'Issuer_Squad', 'Vehicle_Make', 'Sub_Division', 'Intersecting_Street', 'Vehicle_Color', 'Time_First_Observed', 'To_Hours_In_Effect', 'Issuer_Command', 'Date_First_Observed', 'House_Number', 'Violation_Legal_Code', 'No_Standing_or_Stopping_Violation', 'Violation_Description', 'Vehicle_Body_Type']\n"]}],"source":["numeric_columns = list()\n","categorical_column = list()\n","for col_ in dataframe.columns:\n","    if dataframe.select(col_).dtypes[0][1] != \"string\":\n","        numeric_columns.append(col_)\n","    else:\n","        categorical_column.append(col_)\n","        \n","numeric_columns = numeric_columns[1:]\n","print(\"Numeric columns\",numeric_columns)\n","print('-'*100)\n","print(\"categorical columns\",categorical_column)\n"]},{"cell_type":"code","execution_count":16,"id":"a677f8f9","metadata":{"scrolled":true},"outputs":[],"source":["#dataframe.select(numeric_columns[:6]).describe().show()"]},{"cell_type":"code","execution_count":17,"id":"64bf093f","metadata":{},"outputs":[],"source":["#dataframe.select(numeric_columns[6:]).describe().show()"]},{"cell_type":"code","execution_count":18,"id":"21f8fd33","metadata":{},"outputs":[],"source":["dataframe = dataframe.filter(dataframe.Violation_Precinct<=99)"]},{"cell_type":"code","execution_count":19,"id":"761ea1a7","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["23/04/27 21:04:16 WARN org.apache.spark.sql.catalyst.util.package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n","                                                                                \r"]},{"data":{"text/plain":["35"]},"execution_count":19,"metadata":{},"output_type":"execute_result"}],"source":["\n","def drop_null_columns(df):\n","    \"\"\"\n","    This function drops all columns which contain null values.\n","    :param df: A PySpark DataFrame\n","    \"\"\"\n","    n = df.count()\n","    null_counts = df.select([F.count(F.when(F.col(c).isNull(), c)).alias(c) for c in df.columns]).collect()[0].asDict()\n","    to_drop = [k for k, v in null_counts.items() if v >= 0.5*n]\n","    df = df.drop(*to_drop)\n","    return df\n","\n","dataframe = drop_null_columns(dataframe)\n","len(dataframe.columns)"]},{"cell_type":"code","execution_count":20,"id":"066c4cfa","metadata":{},"outputs":[],"source":["#y = dataframe.select('Violation_Precinct')\n","#dataframe = dataframe.drop('Violation_Precinct')"]},{"cell_type":"code","execution_count":21,"id":"0b4c67a2","metadata":{},"outputs":[],"source":["dataframe = dataframe.na.drop(subset=['Violation_Precinct'])"]},{"cell_type":"code","execution_count":22,"id":"16c6a017","metadata":{},"outputs":[],"source":["cols = dataframe.columns\n","fill_dict = {x:-1 for x in cols}\n","dataframe = dataframe.na.fill(fill_dict)"]},{"cell_type":"code","execution_count":23,"id":"5e9bb0b4","metadata":{},"outputs":[],"source":["train,test = dataframe.randomSplit([0.7, 0.3], seed = 0)"]},{"cell_type":"code","execution_count":24,"id":"25b051dc","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Numeric columns ['Violation_Precinct', 'Feet_From_Curb', 'Issuer_Precinct', 'Street_Code2', 'Street_Code1', 'Issuer_Code', 'Street_Code3', 'Violation_Code', 'Vehicle_Year', 'Summons_Number', 'Law_Section', 'Violation_Location']\n","12\n","----------------------------------------------------------------------------------------------------\n","categorical columns ['Violation_Time', 'Violation_In_Front_Of_Or_Opposite', 'From_Hours_In_Effect', 'Issuing_Agency', 'Violation_County', 'Plate_Type', 'Issue_Date', 'Violation_Post_Code', 'Plate_ID', 'Street_Name', 'Registration_State', 'Days_Parking_In_Effect', 'Vehicle_Expiration_Date', 'Issuer_Squad', 'Vehicle_Make', 'Sub_Division', 'Vehicle_Color', 'To_Hours_In_Effect', 'Issuer_Command', 'Date_First_Observed', 'House_Number', 'Violation_Description', 'Vehicle_Body_Type']\n","23\n"]}],"source":["numeric_columns = list()\n","categorical_columns = list()\n","for col_ in dataframe.columns:\n","    if dataframe.select(col_).dtypes[0][1] != \"string\":\n","        numeric_columns.append(col_)\n","    else:\n","        categorical_columns.append(col_)\n","        \n","print(\"Numeric columns\",numeric_columns)\n","print(len(numeric_columns))\n","print('-'*100)\n","print(\"categorical columns\",categorical_columns)\n","print(len(categorical_columns))"]},{"cell_type":"code","execution_count":25,"id":"89b008c2","metadata":{},"outputs":[],"source":["indexed_cols = [i + '_indexed' for i in categorical_columns]"]},{"cell_type":"code","execution_count":26,"id":"ab858f65","metadata":{},"outputs":[],"source":["stringIndexer = StringIndexer(inputCols=categorical_columns+['Violation_Precinct'], outputCols=indexed_cols+['label'],handleInvalid = 'keep')\n","#indexed_df = stringIndexer.transform(train)"]},{"cell_type":"code","execution_count":27,"id":"36c4302d","metadata":{},"outputs":[],"source":["#df = assembled.select('raw_features').toPandas()\n","#dense_udf = udf(lambda x:DenseVector(x.toArray()), VectorUDT())\n","#df = assembled.select('raw_features', dense_udf(\"raw_features\").alias(\"raw_features_udf\"))\n","#df.iloc[0]['raw_features']\n","#df.select('raw_features_udf').toPandas()"]},{"cell_type":"code","execution_count":28,"id":"bb5236a8","metadata":{},"outputs":[],"source":["assembler = VectorAssembler(inputCols=numeric_columns[1:]+indexed_cols,outputCol=\"features\",handleInvalid = 'keep')\n","#assembled = assembler.transform(indexed_df)\n","#assembled = assembled.withColumn('raw_features_udf', dense_udf(\"raw_features\"))\n","#assembled.select('raw_features_udf').show()"]},{"cell_type":"code","execution_count":29,"id":"98b27b55","metadata":{},"outputs":[],"source":["#normalizer = Normalizer(inputCol='raw_features', outputCol=\"norm_features\",p=1.0)\n"]},{"cell_type":"code","execution_count":30,"id":"28e26afc","metadata":{},"outputs":[],"source":["#nb = NaiveBayes()\n","lr = LogisticRegression(maxIter=25, regParam=0.3, elasticNetParam=0.8)\n","#gbt = GBTClassifier(labelCol=\"Violation_Precinct\", featuresCol=\"norm_features\")\n","#rf = RandomForestClassifier(featuresCol=\"features\", labelCol=\"label\", numTrees=512, maxDepth=16)"]},{"cell_type":"code","execution_count":31,"id":"51263f9e","metadata":{},"outputs":[],"source":["pipeline= Pipeline(stages=[stringIndexer,assembler,lr])"]},{"cell_type":"code","execution_count":32,"id":"307674fa","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["23/04/27 21:13:04 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 18.1 MiB\n","23/04/27 21:13:20 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 18.1 MiB\n","23/04/27 21:13:45 WARN com.github.fommil.netlib.BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS\n","23/04/27 21:13:45 WARN com.github.fommil.netlib.BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeRefBLAS\n","23/04/27 21:13:45 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 18.1 MiB\n","23/04/27 21:13:55 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 18.1 MiB\n","23/04/27 21:13:55 WARN org.apache.spark.network.server.TransportChannelHandler: Exception in connection from /10.128.0.22:47350\n","java.io.IOException: Connection reset by peer\n","\tat sun.nio.ch.FileDispatcherImpl.read0(Native Method)\n","\tat sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:39)\n","\tat sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223)\n","\tat sun.nio.ch.IOUtil.read(IOUtil.java:192)\n","\tat sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:379)\n","\tat io.netty.buffer.PooledByteBuf.setBytes(PooledByteBuf.java:253)\n","\tat io.netty.buffer.AbstractByteBuf.writeBytes(AbstractByteBuf.java:1133)\n","\tat io.netty.channel.socket.nio.NioSocketChannel.doReadBytes(NioSocketChannel.java:350)\n","\tat io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:148)\n","\tat io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:714)\n","\tat io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:650)\n","\tat io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:576)\n","\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493)\n","\tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)\n","\tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\n","\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n","\tat java.lang.Thread.run(Thread.java:750)\n","23/04/27 21:13:55 ERROR org.apache.spark.network.client.TransportResponseHandler: Still have 1 requests outstanding when connection from /10.128.0.22:47350 is closed\n","23/04/27 21:13:55 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: Error trying to remove broadcast 18 from block manager BlockManagerId(1, cluster-4b7e-m.c.cs4830-lab2.internal, 36197, None)\n","java.io.IOException: Connection reset by peer\n","\tat sun.nio.ch.FileDispatcherImpl.read0(Native Method)\n","\tat sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:39)\n","\tat sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223)\n","\tat sun.nio.ch.IOUtil.read(IOUtil.java:192)\n","\tat sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:379)\n","\tat io.netty.buffer.PooledByteBuf.setBytes(PooledByteBuf.java:253)\n","\tat io.netty.buffer.AbstractByteBuf.writeBytes(AbstractByteBuf.java:1133)\n","\tat io.netty.channel.socket.nio.NioSocketChannel.doReadBytes(NioSocketChannel.java:350)\n","\tat io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:148)\n","\tat io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:714)\n","\tat io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:650)\n","\tat io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:576)\n","\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:493)\n","\tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)\n","\tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\n","\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n","\tat java.lang.Thread.run(Thread.java:750)\n","23/04/27 21:13:55 ERROR org.apache.spark.network.client.TransportResponseHandler: Still have 1 requests outstanding when connection from /10.128.0.22:47350 is closed\n","23/04/27 21:13:55 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: Error trying to remove broadcast 22 from block manager BlockManagerId(1, cluster-4b7e-m.c.cs4830-lab2.internal, 36197, None)\n","java.io.IOException: Connection from /10.128.0.22:47350 closed\n","\tat org.apache.spark.network.client.TransportResponseHandler.channelInactive(TransportResponseHandler.java:146)\n","\tat org.apache.spark.network.server.TransportChannelHandler.channelInactive(TransportChannelHandler.java:117)\n","\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:262)\n","\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:248)\n","\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelInactive(AbstractChannelHandlerContext.java:241)\n","\tat io.netty.channel.ChannelInboundHandlerAdapter.channelInactive(ChannelInboundHandlerAdapter.java:81)\n","\tat io.netty.handler.timeout.IdleStateHandler.channelInactive(IdleStateHandler.java:277)\n","\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:262)\n","\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:248)\n","\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelInactive(AbstractChannelHandlerContext.java:241)\n","\tat io.netty.channel.ChannelInboundHandlerAdapter.channelInactive(ChannelInboundHandlerAdapter.java:81)\n","\tat org.apache.spark.network.util.TransportFrameDecoder.channelInactive(TransportFrameDecoder.java:225)\n","\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:262)\n","\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:248)\n","\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelInactive(AbstractChannelHandlerContext.java:241)\n","\tat io.netty.channel.DefaultChannelPipeline$HeadContext.channelInactive(DefaultChannelPipeline.java:1405)\n","\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:262)\n","\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:248)\n","\tat io.netty.channel.DefaultChannelPipeline.fireChannelInactive(DefaultChannelPipeline.java:901)\n","\tat io.netty.channel.AbstractChannel$AbstractUnsafe$8.run(AbstractChannel.java:818)\n","\tat io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:164)\n","\tat io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:472)\n","\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:497)\n","\tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)\n","\tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\n","\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n","\tat java.lang.Thread.run(Thread.java:750)\n","23/04/27 21:13:55 ERROR org.apache.spark.network.client.TransportClient: Failed to send RPC RPC 7285355634646838684 to /10.128.0.22:47350: java.nio.channels.ClosedChannelException\n","java.nio.channels.ClosedChannelException\n","\tat io.netty.channel.AbstractChannel$AbstractUnsafe.newClosedChannelException(AbstractChannel.java:957)\n","\tat io.netty.channel.AbstractChannel$AbstractUnsafe.write(AbstractChannel.java:865)\n","\tat io.netty.channel.DefaultChannelPipeline$HeadContext.write(DefaultChannelPipeline.java:1367)\n","\tat io.netty.channel.AbstractChannelHandlerContext.invokeWrite0(AbstractChannelHandlerContext.java:717)\n","\tat io.netty.channel.AbstractChannelHandlerContext.invokeWriteAndFlush(AbstractChannelHandlerContext.java:764)\n","\tat io.netty.channel.AbstractChannelHandlerContext.write(AbstractChannelHandlerContext.java:790)\n","\tat io.netty.channel.AbstractChannelHandlerContext.writeAndFlush(AbstractChannelHandlerContext.java:758)\n","\tat io.netty.channel.AbstractChannelHandlerContext.invokeWriteAndFlush(AbstractChannelHandlerContext.java:767)\n","\tat io.netty.channel.AbstractChannelHandlerContext.write(AbstractChannelHandlerContext.java:790)\n","\tat io.netty.channel.AbstractChannelHandlerContext.writeAndFlush(AbstractChannelHandlerContext.java:758)\n","\tat io.netty.channel.AbstractChannelHandlerContext.invokeWriteAndFlush(AbstractChannelHandlerContext.java:767)\n","\tat io.netty.channel.AbstractChannelHandlerContext$WriteTask.run(AbstractChannelHandlerContext.java:1071)\n","\tat io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:164)\n","\tat io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:472)\n","\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:497)\n","\tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:989)\n","\tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\n","\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n","\tat java.lang.Thread.run(Thread.java:750)\n","23/04/27 21:13:55 WARN org.apache.spark.rpc.netty.NettyRpcEnv: Ignored failure: java.io.IOException: Failed to send RPC RPC 7285355634646838684 to /10.128.0.22:47350: java.nio.channels.ClosedChannelException\n","23/04/27 21:13:55 WARN org.apache.spark.storage.BlockManagerMasterEndpoint: No more replicas available for rdd_56_0 !\n","23/04/27 21:13:56 WARN org.apache.spark.deploy.yarn.YarnAllocator: Container from a bad node: container_1682629215013_0001_01_000001 on host: cluster-4b7e-m.c.cs4830-lab2.internal. Exit status: 137. Diagnostics: [2023-04-27 21:13:56.107]Container killed on request. Exit code is 137\n","[2023-04-27 21:13:56.110]Container exited with a non-zero exit code 137. \n","[2023-04-27 21:13:56.124]Killed by external signal\n",".\n","23/04/27 21:13:56 WARN org.apache.spark.scheduler.cluster.YarnSchedulerBackend$YarnSchedulerEndpoint: Requesting driver to remove executor 1 for reason Container from a bad node: container_1682629215013_0001_01_000001 on host: cluster-4b7e-m.c.cs4830-lab2.internal. Exit status: 137. Diagnostics: [2023-04-27 21:13:56.107]Container killed on request. Exit code is 137\n","[2023-04-27 21:13:56.110]Container exited with a non-zero exit code 137. \n","[2023-04-27 21:13:56.124]Killed by external signal\n",".\n","23/04/27 21:13:56 ERROR org.apache.spark.scheduler.cluster.YarnScheduler: Lost executor 1 on cluster-4b7e-m.c.cs4830-lab2.internal: Container from a bad node: container_1682629215013_0001_01_000001 on host: cluster-4b7e-m.c.cs4830-lab2.internal. Exit status: 137. Diagnostics: [2023-04-27 21:13:56.107]Container killed on request. Exit code is 137\n","[2023-04-27 21:13:56.110]Container exited with a non-zero exit code 137. \n","[2023-04-27 21:13:56.124]Killed by external signal\n",".\n","23/04/27 21:13:56 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 0.0 in stage 21.0 (TID 407) (cluster-4b7e-m.c.cs4830-lab2.internal executor 1): ExecutorLostFailure (executor 1 exited caused by one of the running tasks) Reason: Container from a bad node: container_1682629215013_0001_01_000001 on host: cluster-4b7e-m.c.cs4830-lab2.internal. Exit status: 137. Diagnostics: [2023-04-27 21:13:56.107]Container killed on request. Exit code is 137\n","[2023-04-27 21:13:56.110]Container exited with a non-zero exit code 137. \n","[2023-04-27 21:13:56.124]Killed by external signal\n",".\n","23/04/27 21:14:19 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 18.1 MiB\n","23/04/27 21:14:29 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 18.1 MiB\n","23/04/27 21:14:39 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 18.1 MiB\n","23/04/27 21:14:49 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 18.1 MiB\n","23/04/27 21:14:59 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 18.1 MiB\n","23/04/27 21:15:09 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 18.1 MiB\n","23/04/27 21:15:19 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 18.1 MiB\n","23/04/27 21:15:29 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 18.1 MiB\n","23/04/27 21:15:39 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 18.1 MiB\n","23/04/27 21:15:49 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 18.1 MiB\n","23/04/27 21:15:59 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 18.1 MiB\n","23/04/27 21:16:09 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 18.1 MiB\n","23/04/27 21:16:19 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 18.1 MiB\n","23/04/27 21:16:29 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 18.1 MiB\n","23/04/27 21:16:39 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 18.1 MiB\n","23/04/27 21:16:48 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 18.1 MiB\n","23/04/27 21:16:58 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 18.1 MiB\n","23/04/27 21:17:08 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 18.1 MiB\n","23/04/27 21:17:18 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 18.1 MiB\n","23/04/27 21:17:28 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 18.1 MiB\n","23/04/27 21:17:38 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 18.1 MiB\n","23/04/27 21:17:48 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 18.1 MiB\n","23/04/27 21:17:58 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 18.1 MiB\n","23/04/27 21:18:08 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 18.1 MiB\n","23/04/27 21:18:18 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 18.1 MiB\n","23/04/27 21:18:27 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 18.1 MiB\n","23/04/27 21:18:37 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 18.1 MiB\n","23/04/27 21:18:47 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 18.1 MiB\n","23/04/27 21:18:57 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 18.1 MiB\n","23/04/27 21:19:07 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 18.1 MiB\n","23/04/27 21:19:16 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 18.1 MiB\n","23/04/27 21:19:26 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 18.1 MiB\n","23/04/27 21:19:36 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 18.1 MiB\n","23/04/27 21:19:46 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 18.1 MiB\n","23/04/27 21:19:56 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 18.1 MiB\n","23/04/27 21:20:06 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 18.1 MiB\n","23/04/27 21:20:15 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 18.1 MiB\n","23/04/27 21:20:25 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 18.1 MiB\n","23/04/27 21:20:35 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 18.1 MiB\n","23/04/27 21:20:45 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 18.1 MiB\n","23/04/27 21:20:55 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 18.1 MiB\n","23/04/27 21:21:05 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 18.1 MiB\n","23/04/27 21:21:15 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 18.1 MiB\n","23/04/27 21:21:24 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 18.1 MiB\n","23/04/27 21:21:34 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 18.1 MiB\n","23/04/27 21:21:44 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 18.1 MiB\n","23/04/27 21:23:10 WARN org.apache.spark.deploy.yarn.YarnAllocator: Container from a bad node: container_1682629215013_0001_01_000002 on host: cluster-4b7e-m.c.cs4830-lab2.internal. Exit status: 137. Diagnostics: [2023-04-27 21:23:10.736]Container killed on request. Exit code is 137\n","[2023-04-27 21:23:10.736]Container exited with a non-zero exit code 137. \n","[2023-04-27 21:23:10.737]Killed by external signal\n",".\n","23/04/27 21:23:10 WARN org.apache.spark.scheduler.cluster.YarnSchedulerBackend$YarnSchedulerEndpoint: Requesting driver to remove executor 2 for reason Container from a bad node: container_1682629215013_0001_01_000002 on host: cluster-4b7e-m.c.cs4830-lab2.internal. Exit status: 137. Diagnostics: [2023-04-27 21:23:10.736]Container killed on request. Exit code is 137\n","[2023-04-27 21:23:10.736]Container exited with a non-zero exit code 137. \n","[2023-04-27 21:23:10.737]Killed by external signal\n",".\n","23/04/27 21:23:10 ERROR org.apache.spark.scheduler.cluster.YarnScheduler: Lost executor 2 on cluster-4b7e-m.c.cs4830-lab2.internal: Container from a bad node: container_1682629215013_0001_01_000002 on host: cluster-4b7e-m.c.cs4830-lab2.internal. Exit status: 137. Diagnostics: [2023-04-27 21:23:10.736]Container killed on request. Exit code is 137\n","[2023-04-27 21:23:10.736]Container exited with a non-zero exit code 137. \n","[2023-04-27 21:23:10.737]Killed by external signal\n",".\n","23/04/27 21:23:10 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 25.0 in stage 114.0 (TID 480) (cluster-4b7e-m.c.cs4830-lab2.internal executor 2): ExecutorLostFailure (executor 2 exited caused by one of the running tasks) Reason: Container from a bad node: container_1682629215013_0001_01_000002 on host: cluster-4b7e-m.c.cs4830-lab2.internal. Exit status: 137. Diagnostics: [2023-04-27 21:23:10.736]Container killed on request. Exit code is 137\n","[2023-04-27 21:23:10.736]Container exited with a non-zero exit code 137. \n","[2023-04-27 21:23:10.737]Killed by external signal\n",".\n","23/04/27 21:23:10 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 27.0 in stage 114.0 (TID 482) (cluster-4b7e-m.c.cs4830-lab2.internal executor 2): ExecutorLostFailure (executor 2 exited caused by one of the running tasks) Reason: Container from a bad node: container_1682629215013_0001_01_000002 on host: cluster-4b7e-m.c.cs4830-lab2.internal. Exit status: 137. Diagnostics: [2023-04-27 21:23:10.736]Container killed on request. Exit code is 137\n","[2023-04-27 21:23:10.736]Container exited with a non-zero exit code 137. \n","[2023-04-27 21:23:10.737]Killed by external signal\n",".\n","[Stage 114:=====================================================> (49 + 1) / 50]\r"]}],"source":["model = pipeline.fit(train)"]},{"cell_type":"code","execution_count":33,"id":"9aac329d","metadata":{},"outputs":[],"source":["#model = rf.fit(assembled)"]},{"cell_type":"code","execution_count":34,"id":"83206e13","metadata":{},"outputs":[{"data":{"text/plain":["'paramGrid = ParamGridBuilder().addGrid(gbt.maxIter, [10,10,50]).build()\\ncrossval = CrossValidator(estimator=pipeline,estimatorParamMaps=paramGrid,evaluator=MulticlassClassificationEvaluator(),numFolds=4)\\n# Run cross-validation, and choose the best set of parameters.\\ncvModel = crossval.fit(train)\\nprediction = cvModel.transform(test)\\nselected = prediction.select(\"label\", \"probability\", \"prediction\")\\nprint(MulticlassClassificationEvaluator(metricName =\"accuracy\").evaluate(prediction))'"]},"execution_count":34,"metadata":{},"output_type":"execute_result"}],"source":["'''paramGrid = ParamGridBuilder().addGrid(gbt.maxIter, [10,10,50]).build()\n","crossval = CrossValidator(estimator=pipeline,estimatorParamMaps=paramGrid,evaluator=MulticlassClassificationEvaluator(),numFolds=4)\n","# Run cross-validation, and choose the best set of parameters.\n","cvModel = crossval.fit(train)\n","prediction = cvModel.transform(test)\n","selected = prediction.select(\"label\", \"probability\", \"prediction\")\n","print(MulticlassClassificationEvaluator(metricName =\"accuracy\").evaluate(prediction))'''"]},{"cell_type":"code","execution_count":35,"id":"7bdec01f","metadata":{},"outputs":[{"data":{"text/plain":["'test = stringIndexer.transform(test)\\ntest = assembler.transform(test)\\ntest = test.withColumn(\\'raw_features_udf\\', dense_udf(\"raw_features\"))'"]},"execution_count":35,"metadata":{},"output_type":"execute_result"}],"source":["'''test = stringIndexer.transform(test)\n","test = assembler.transform(test)\n","test = test.withColumn('raw_features_udf', dense_udf(\"raw_features\"))'''\n"]},{"cell_type":"code","execution_count":36,"id":"a81ae0d3","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["23/04/27 21:25:49 WARN org.apache.spark.scheduler.DAGScheduler: Broadcasting large task binary with size 18.1 MiB\n","[Stage 117:>                                                        (0 + 1) / 1]\r"]},{"name":"stdout","output_type":"stream","text":["Accuracy of model is  0.2019286125503742\n"]},{"name":"stderr","output_type":"stream","text":["                                                                                \r"]}],"source":["predictions = model.transform(test)\n","#predictions.select(['target', 'rawPrediction','probability','prediction']).show()\n","evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n","accuracy = evaluator.evaluate(predictions)\n","print(\"Accuracy of model is \", accuracy)"]},{"cell_type":"code","execution_count":37,"id":"d92297fe","metadata":{},"outputs":[{"data":{"text/plain":["['Violation_Precinct',\n"," 'Feet_From_Curb',\n"," 'Violation_Time',\n"," 'Violation_In_Front_Of_Or_Opposite',\n"," 'Issuer_Precinct',\n"," 'Street_Code2',\n"," 'From_Hours_In_Effect',\n"," 'Issuing_Agency',\n"," 'Street_Code1',\n"," 'Issuer_Code',\n"," 'Violation_County',\n"," 'Plate_Type',\n"," 'Issue_Date',\n"," 'Violation_Post_Code',\n"," 'Street_Code3',\n"," 'Plate_ID',\n"," 'Violation_Code',\n"," 'Street_Name',\n"," 'Registration_State',\n"," 'Days_Parking_In_Effect',\n"," 'Vehicle_Expiration_Date',\n"," 'Issuer_Squad',\n"," 'Vehicle_Make',\n"," 'Sub_Division',\n"," 'Vehicle_Year',\n"," 'Vehicle_Color',\n"," 'Summons_Number',\n"," 'Law_Section',\n"," 'Violation_Location',\n"," 'To_Hours_In_Effect',\n"," 'Issuer_Command',\n"," 'Date_First_Observed',\n"," 'House_Number',\n"," 'Violation_Description',\n"," 'Vehicle_Body_Type',\n"," 'Violation_Time_indexed',\n"," 'Violation_In_Front_Of_Or_Opposite_indexed',\n"," 'From_Hours_In_Effect_indexed',\n"," 'Issuing_Agency_indexed',\n"," 'Violation_County_indexed',\n"," 'Plate_Type_indexed',\n"," 'Issue_Date_indexed',\n"," 'Violation_Post_Code_indexed',\n"," 'Plate_ID_indexed',\n"," 'Street_Name_indexed',\n"," 'Registration_State_indexed',\n"," 'Days_Parking_In_Effect_indexed',\n"," 'Vehicle_Expiration_Date_indexed',\n"," 'Issuer_Squad_indexed',\n"," 'Vehicle_Make_indexed',\n"," 'Sub_Division_indexed',\n"," 'Vehicle_Color_indexed',\n"," 'To_Hours_In_Effect_indexed',\n"," 'Issuer_Command_indexed',\n"," 'Date_First_Observed_indexed',\n"," 'House_Number_indexed',\n"," 'Violation_Description_indexed',\n"," 'Vehicle_Body_Type_indexed',\n"," 'label',\n"," 'features',\n"," 'rawPrediction',\n"," 'probability',\n"," 'prediction']"]},"execution_count":37,"metadata":{},"output_type":"execute_result"}],"source":["predictions.columns"]},{"cell_type":"code","execution_count":39,"id":"4e35a4e5","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["23/04/27 21:27:18 WARN org.apache.spark.scheduler.TaskSetManager: Stage 121 contains a task of very large size (7295 KiB). The maximum recommended task size is 1000 KiB.\n","                                                                                \r"]}],"source":["from google.cloud import storage\n","\n","srcPath = 'gs://mm19b059_1/logistic_regression.model'\n","model.write().overwrite().save(srcPath)"]},{"cell_type":"code","execution_count":null,"id":"1615c5f1","metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.15"}},"nbformat":4,"nbformat_minor":5}